{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Num GPUs Available: 1\n",
      "Name: /physical_device:GPU:0, Type: GPU\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Add parent directory to path.\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413\n",
    "fdir = Path(os.path.abspath('')).resolve() # Directory of current file.\n",
    "path = fdir/'..'\n",
    "if path not in sys.path:\n",
    "    sys.path.append(str(path))\n",
    "\n",
    "# Complete imports.\n",
    "from functools import partial\n",
    "import makassar_ml as ml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sns.set() # Use seaborn themes.\n",
    "\n",
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_roots = dict(\n",
    "    hp_tuning_root=str(Path('~/research/makassar/hp_tuning').expanduser()),\n",
    ")\n",
    "config_dataset = dict(\n",
    "    parameters=dict(\n",
    "        path=str(Path('~/research/makassar/datasets/beijing_pm25').expanduser()),\n",
    "        in_feat=['day_of_year','TEMP','Iws','Is','Ir'],\n",
    "        out_feat=['DEWP','PRES'],\n",
    "        in_seq_len=24*30, # hours\n",
    "        out_seq_len=1,\n",
    "        shift=1,\n",
    "        split=[0.7,0.2,0.1],\n",
    "        shuffle=False,\n",
    "    ),\n",
    ")\n",
    "config_model=dict(\n",
    "    name='FoT',\n",
    "    parameters=dict(\n",
    "        in_seq_len=config_dataset['parameters']['in_seq_len'],\n",
    "        in_feat=len(config_dataset['parameters']['in_feat']),\n",
    "        out_feat=len(config_dataset['parameters']['out_feat']),\n",
    "        embed_dim=dict(\n",
    "            values=[\n",
    "                16,\n",
    "                32,\n",
    "            ],\n",
    "        ),\n",
    "        n_heads=dict(\n",
    "            values=[\n",
    "                8,\n",
    "            ],\n",
    "        ),\n",
    "        ff_dim=dict(\n",
    "            values=[\n",
    "                256,\n",
    "                512,\n",
    "            ],\n",
    "        ),\n",
    "        dropout=dict(\n",
    "            values=[\n",
    "                0.1,\n",
    "                0.3,\n",
    "            ],\n",
    "        ),\n",
    "        n_encoders=dict(\n",
    "            values=[\n",
    "                3,\n",
    "                6,\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "config_train=dict(\n",
    "    batch_size=128,\n",
    "    epochs=30,\n",
    "    optimizer=dict(\n",
    "        name='adam',\n",
    "        parameters=dict(\n",
    "            lr=0.001,\n",
    "        ),\n",
    "    ),\n",
    "    compile=dict(\n",
    "        loss='mse',\n",
    "        metrics=['mae','mape'],\n",
    "    ),\n",
    "    callbacks=dict(\n",
    "        EarlyStopping=dict(\n",
    "            monitor='val_loss',\n",
    "            mode='auto',\n",
    "            patience=50,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "config=dict(\n",
    "    roots=config_roots,\n",
    "    dataset=config_dataset,\n",
    "    model=config_model,\n",
    "    train=config_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid size: 16\n"
     ]
    }
   ],
   "source": [
    "# Convert config into parameter dictionary.\n",
    "parameterdict = ml.tuning.config2parameterdict(config)\n",
    "\n",
    "# Count number of grid combinations.\n",
    "grid = ParameterGrid(ml.tuning.config2parameterdict(config))\n",
    "print(f\"Parameter grid size: {len(grid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader_func(\n",
    "    batch_size: int,\n",
    "    ) -> tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "        return ml.datasets.beijingpm25.load_beijingpm25_ds(\n",
    "            **config['dataset']['parameters'],\n",
    "            batch_size=batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_func(params: dict) -> keras.Model:\n",
    "    # Create copy of parameter dictionary.\n",
    "    model_params = dict(**params)\n",
    "\n",
    "    # Configure optimizer.\n",
    "    optimizer_config = dict()\n",
    "    if 'lr' in model_params:\n",
    "        optimizer_config['lr'] = model_params.pop('lr')\n",
    "    optimizer_class_name = model_params.pop('optimizer')\n",
    "    optim = keras.optimizers.get({\n",
    "    'class_name': optimizer_class_name,\n",
    "        'config': optimizer_config,\n",
    "    })\n",
    "\n",
    "    # Get build function for specific model.\n",
    "    build_model = getattr(getattr(\n",
    "        ml.models,\n",
    "        config['model']['name'].lower(),\n",
    "        ),\n",
    "        config['model']['name'],\n",
    "        )\n",
    "    model = build_model(\n",
    "        **model_params,\n",
    "    )\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optim,\n",
    "        **config['train']['compile'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training strategy.\n",
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback list if any were specified.\n",
    "callbacks = []\n",
    "if 'callbacks' in config['train']:\n",
    "    for key, cb_params in config['train']['callbacks'].items():\n",
    "        if hasattr(ml.callbacks, key):\n",
    "            callbacks.append(getattr(ml.callbacks, key)(**cb_params))\n",
    "        elif hasattr(keras.callbacks, key):\n",
    "            if key == 'LearningRateScheduler':\n",
    "                callbacks.append(\n",
    "                    keras.callbacks.LearningRateScheduler(\n",
    "                        partial(\n",
    "                            getattr(ml.schedules, cb_params['schedule']),\n",
    "                            **cb_params.get('parameters', {})\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                callbacks.append(\n",
    "                    getattr(keras.callbacks, key)(**cb_params)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/5k/47hxnf0d2v1cvrggbx8z9ts40000gn/T/ipykernel_4444/76650699.py\", line 2, in <cell line: 2>\n",
      "    model, hist, met, params, df = ml.tuning.hp_gridsearch(\n",
      "  File \"/Volumes/GoogleDrive/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/jupyter/../makassar_ml/tuning.py\", line 211, in hp_gridsearch\n",
      "    model = build_model_func(p)\n",
      "  File \"/var/folders/5k/47hxnf0d2v1cvrggbx8z9ts40000gn/T/ipykernel_4444/913067817.py\", line 16, in build_model_func\n",
      "    build_model = getattr(\n",
      "AttributeError: module 'makassar_ml.models' has no attribute 'fot.FoT'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/envs/ml-metal/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model.\n",
    "model, hist, met, params, df = ml.tuning.hp_gridsearch(\n",
    "    model_name=config['model']['name'].lower(),\n",
    "    params=parameterdict,\n",
    "    build_model_func=build_model_func,\n",
    "    dataset_loader_func=dataset_loader_func,\n",
    "    metric_list=config['train']['compile']['metrics'],\n",
    "    batch_size=config['train']['batch_size'],\n",
    "    strategy=strategy,\n",
    "    epochs=config['train']['epochs'],\n",
    "    tuning_root=config['roots']['hp_tuning_root'],\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/GoogleDrive/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/jupyter/tune_fot.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/research/makassar/repos/makassar-ml/jupyter/tune_fot.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39m# Display best model information.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/GoogleDrive/My%20Drive/Virginia%20Tech/graduate/research/makassar/repos/makassar-ml/jupyter/tune_fot.ipynb#ch0000003?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Display best model information.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tuning Results:\")\n",
    "# Build the resulting table header.\n",
    "table_header = ['model']\n",
    "for m in ['loss']+config['train']['compile']['metrics']:\n",
    "    table_header.append(f\"{m}\")\n",
    "    table_header.append(f\"val_{m}\")\n",
    "    table_header.append(f\"test_{m}\")\n",
    "table_header.extend(list(parameterdict))\n",
    "# Log results as CSV to console.\n",
    "csv_df = df[table_header].sort_values(by='val_loss', ascending=True)\n",
    "print(csv_df.to_string(index=False))\n",
    "# Log results as CSV to file.\n",
    "csv_df.to_csv(\n",
    "    Path(config['roots']['hp_tuning_root'])/config['model']['name'].lower()/f\"tuning_results.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml-metal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10e18adb41d97bf94c5a63a297ea28f8776866e32b392ea8a12a501abb089aed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
