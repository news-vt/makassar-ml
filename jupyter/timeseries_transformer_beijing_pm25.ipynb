{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import glob\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section contains code that is modifies output path locations, random seed, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path('/Users/derieux/research/makassar/datasets').expanduser()\n",
    "if not DATASET_ROOT.exists(): raise ValueError(f\"Dataset root directory does not exist at {DATASET_ROOT}\")\n",
    "PROJECT_ROOT = Path('/Users/derieux/research/makassar').expanduser()\n",
    "CHECKPOINT_ROOT = PROJECT_ROOT / 'checkpoints'\n",
    "IMAGE_ROOT = PROJECT_ROOT / 'images'\n",
    "\n",
    "# Ensure some directories exist.\n",
    "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds.\n",
    "SEED = 0\n",
    "tf.random.set_seed(SEED) # Only this works on ARC (since tensorflow==2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging (useful for ARC systems).\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG) # Must be lowest of all handlers listed below.\n",
    "while logger.hasHandlers(): logger.removeHandler(logger.handlers[0]) # Clear all existing handlers.\n",
    "\n",
    "# Custom log formatting.\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "# Log to STDOUT (uses default formatting).\n",
    "sh = logging.StreamHandler(stream=sys.stdout)\n",
    "sh.setLevel(logging.INFO)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "# Set Tensorflow logging level.\n",
    "tf.get_logger().setLevel('ERROR') # 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# List all GPUs visible to TensorFlow.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "logger.info(f\"Num GPUs Available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    logger.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Subset Loader\n",
    "\n",
    "The `WindowGenerator` class operates on `pandas.DataFrame` objects. It accomplishes several tasks:\n",
    "\n",
    "1. Split the data into contiguous windows.\n",
    "2. Convert the data into a `tensorflow.Dataset` class for batch loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing\n",
    "class WindowGenerator:\n",
    "    def __init__(self,\n",
    "        in_seq_len: int,\n",
    "        out_seq_len: int,\n",
    "        shift: int,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        in_feat: list[str] = None,\n",
    "        out_feat: list[str] = None,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        ):\n",
    "        \"\"\"Constructs sliding windows of sequential data.\n",
    "\n",
    "        Data must already be split into train/val/test subsets,\n",
    "        and provided as `pandas.DataFrame` objects.\n",
    "\n",
    "        Args:\n",
    "            in_seq_len (int): Input sequence length.\n",
    "            out_seq_len (int): Output (target) sequence length.\n",
    "            shift (int): Number of indices to skip between elements when traversing window.\n",
    "            train_df (pd.DataFrame): Training data frame.\n",
    "            val_df (pd.DataFrame): Validation data frame.\n",
    "            test_df (pd.DataFrame): Testing data frame.\n",
    "            in_feat (list[str], optional): Desired subset of input features for window. Defaults to None.\n",
    "            out_feat (list[str], optional): Desired subset of output features for window. Defaults to None.\n",
    "            batch_size (int, optional): Batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Preserve dataframes.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Preserve sequence information.\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.shift = shift\n",
    "        self.total_window_len = in_seq_len + shift\n",
    "\n",
    "        # Setup indexing slices for window extraction.\n",
    "        self.in_slice = slice(0, self.in_seq_len)\n",
    "        self.out_slice = slice(self.total_window_len - self.out_seq_len, None)\n",
    "        self.in_idx = np.arange(self.total_window_len)[self.in_slice]\n",
    "        self.out_idx = np.arange(self.total_window_len)[self.out_slice]\n",
    "\n",
    "        # Setup train/val/test column extractors.\n",
    "        self.col2idx = {name: i for i, name in enumerate(train_df.columns)}\n",
    "        if in_feat is not None:\n",
    "            self.in_feat = in_feat\n",
    "            self.in_col_idx = [self.col2idx[col] for col in in_feat]\n",
    "        else:\n",
    "            self.in_col_idx = list(range(len(train_df.columns)))\n",
    "            self.in_feat = [train_df.columns[i] for i in self.in_col_idx]\n",
    "        if out_feat is not None:\n",
    "            self.out_feat = out_feat\n",
    "            self.out_col_idx = [self.col2idx[col] for col in out_feat]\n",
    "        else:\n",
    "            self.out_col_idx = list(range(len(train_df.columns)))\n",
    "            self.out_feat = [train_df.columns[i] for i in self.out_col_idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation of class.\"\"\"\n",
    "        return '\\n'.join([\n",
    "            f\"Total window length: {self.total_window_len}\",\n",
    "            f\"Input indices: {self.in_idx}\",\n",
    "            f\"Output indices: {self.out_idx}\",\n",
    "            f\"Input features: {self.in_feat}\",\n",
    "            f\"Output features: {self.out_feat}\",\n",
    "        ])\n",
    "\n",
    "    def split_window(self, \n",
    "        window: tf.Tensor, # window shape is (batch, seq, feat)\n",
    "        ) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Splits a single window of data into input/output seqments.\n",
    "\n",
    "        Args:\n",
    "            window (tf.Tensor): Tensor of window data with shape (batch, seq, feat).\n",
    "\n",
    "        Returns:\n",
    "            tuple[tf.Tensor, tf.Tensor]: 2-tuple of input/output data segments, where the shapes are:\n",
    "                - Input window: (batch, in_seq_len, in_feat)\n",
    "                - Output window: (batch, out_seq_len, out_feat)\n",
    "        \"\"\"\n",
    "        # Decompose input/output sequence from given input window.\n",
    "        in_seq = tf.stack([window[:, self.in_slice, i] for i in self.in_col_idx], axis=-1)\n",
    "        out_seq = tf.stack([window[:, self.out_slice, i] for i in self.out_col_idx], axis=-1)\n",
    "\n",
    "        # Set shape for input/output sequences.\n",
    "        # Note that dimensions set to `None` are not updated.\n",
    "        in_seq = tf.ensure_shape(in_seq, (None, self.in_seq_len, None))\n",
    "        out_seq = tf.ensure_shape(out_seq, (None, self.out_seq_len, None))\n",
    "\n",
    "        return in_seq, out_seq\n",
    "\n",
    "    def make_dataset(self, \n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        ) -> tf.data.Dataset:\n",
    "        \"\"\"Construct a TensorFlow Dataset from given input data frame.\n",
    "\n",
    "        Datasets load tuples of batched input/output windows with shapes:\n",
    "            - Input window: (batch, in_seq_len, in_feat)\n",
    "            - Output window: (batch, out_seq_len, out_feat)\n",
    "\n",
    "        Note that output windows are generally target sequences.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Source data frame.\n",
    "            batch_size (int, optional): Batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Shuffle windows prior to batching. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tf.data.Dataset: Dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert data frame into numpy matrix.\n",
    "        data = df.to_numpy()\n",
    "\n",
    "        # Convert data matrix into TensorFlow dataset.\n",
    "        # dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_len,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Pipe the raw dataset into the window splitting function.\n",
    "        dataset = dataset.map(self.split_window)\n",
    "\n",
    "        # Return the dataset.\n",
    "        return dataset\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        \"\"\"Training dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.train_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        \"\"\"Validation dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.val_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        \"\"\"Testing dataset.\"\"\"\n",
    "        return self.make_dataset(\n",
    "            df=self.test_df,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beijing PM2.5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset_df(\n",
    "    df: pd.DataFrame,\n",
    "    split: tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split a Pandas dataframe into train, validation, and test subsets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Source dataframe.\n",
    "        split (tuple[float, float, float], optional): Tuple of split ratios. Must sum to 1. Defaults to (0.8, 0.1, 0.1).\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Tuple of train, validation, and test dataframes.\n",
    "    \"\"\"\n",
    "    assert isinstance(split, (list, tuple))\n",
    "    assert len(split) == 3\n",
    "    assert sum(split) == 1.0\n",
    "\n",
    "    # Split dataframe into train/val/test dataframes.\n",
    "    # Inspiration: https://www.tensorflow.org/tutorials/structured_data/time_series#split_the_data\n",
    "    train_split, val_split, test_split = split # Unpack split tuple.\n",
    "    n = len(df.index) # Total number of data records.\n",
    "    df_train = df[:int(n*train_split)].copy()\n",
    "    df_val = df[int(n*train_split):int(n*(1-test_split))].copy()\n",
    "    df_test = df[int(n*(1-test_split)):].copy()\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_beijingpm25_df(\n",
    "    path: str = None,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Loads Beijing PM2.5 dataset as a pandas dataframe.\n",
    "\n",
    "    https://archive-beta.ics.uci.edu/ml/datasets/beijing+pm2+5+data\n",
    "\n",
    "    Dataset features:\n",
    "        - `No`: (NOT USED) row number\n",
    "        - `year`: year of data in this row\n",
    "        - `month`: month of data in this row\n",
    "        - `day`: day of data in this row\n",
    "        - `hour`: hour of data in this row\n",
    "        - `pm2.5`: PM2.5 concentration (ug/m^3)\n",
    "        - `DEWP`: Dew Point (â„ƒ)\n",
    "        - `TEMP`: Temperature (â„ƒ)\n",
    "        - `PRES`: Pressure (hPa)\n",
    "        - `cbwd`: (NOT USED) Combined wind direction\n",
    "        - `Iws`: Cumulated wind speed (m/s)\n",
    "        - `Is`: Cumulated hours of snow\n",
    "        - `Ir`: Cumulated hours of rain\n",
    "        - `datetime`: (NOT USED) dynamically generated datetime string\n",
    "    \"\"\"\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv'\n",
    "\n",
    "    # Set path to current directory if necessary.\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "\n",
    "    # Convert path to `Path` object.\n",
    "    if not isinstance(path, Path):\n",
    "        path = Path(path)\n",
    "\n",
    "    # Create path if necessary.\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download the dataset if necessary.\n",
    "    filename = Path(url.rsplit('/', 1)[1]) # Get name of file.\n",
    "    filepath = (path / filename).expanduser().resolve()\n",
    "    filepath = keras.utils.get_file(\n",
    "        fname=filepath,\n",
    "        origin=url,\n",
    "        )\n",
    "\n",
    "    # Load as Pandas DataFrame.\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Create single date column from independent year/month/day columns.\n",
    "    df['datetime'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_beijingpm25_df(DATASET_ROOT/'beijing_pm25')\n",
    "print(df.info())\n",
    "\n",
    "a, b, c = partition_dataset_df(df)\n",
    "print(a.info())\n",
    "print(b.info())\n",
    "print(c.info())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e27c2da514be0f4555df3a4c15a4c6256ef40203ad64abea68e2343d203af1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
