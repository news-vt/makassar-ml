{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hqTzrIYbiSfZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfK265ryiSfc",
        "outputId": "5fbffb32-3cd1-4d1d-be13-f1926edea2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOT IN COLAB\n"
          ]
        }
      ],
      "source": [
        "# Detect if running in Google Colab environment.\n",
        "# If so, then clone/install package from GitHub.\n",
        "# Otherwise, use locally.\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print('IN COLAB')\n",
        "    \n",
        "    config_path = pathlib.Path('./config.json')\n",
        "    if config_path.exists():\n",
        "        import json\n",
        "        with open(config_path, 'r') as f:\n",
        "            j = json.load(f)\n",
        "        os.environ['GITHUB_TOKEN'] = j['GITHUB_TOKEN']\n",
        "\n",
        "    else:\n",
        "\n",
        "        # Request GitHub access token.\n",
        "        if os.getenv('GITHUB_TOKEN',None) is None:\n",
        "            os.environ['GITHUB_TOKEN'] = getpass.getpass('GitHub Token: ')\n",
        "        else:\n",
        "            print('Using cached GitHub Token')\n",
        "        \n",
        "        import json\n",
        "        with open(config_path, 'w+') as f:\n",
        "            json.dump(\n",
        "                {'GITHUB_TOKEN': os.environ['GITHUB_TOKEN']}, \n",
        "                f,\n",
        "            )\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    # Clone or update repo.\n",
        "    repo = \"makassar-ml\"\n",
        "    repo_url = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/news-vt/{repo}.git\"\n",
        "    repo_path = f\"/content/{repo}\"\n",
        "    repo_branch = \"develop\"\n",
        "    ![ -d $repo_path ] && git -C $repo_path pull || git clone --branch $repo_branch $repo_url\n",
        "    # !git clone --branch $repo_branch $repo_url\n",
        "\n",
        "    # Install repo to ensure dependencies are resolved.\n",
        "    !pip install --upgrade $repo_path\n",
        "\n",
        "    # Add package location to path.\n",
        "    sys.path.insert(0, repo_path)\n",
        "\n",
        "    # Set dataset root path.\n",
        "    dataset_root = pathlib.Path('./dataset')\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    dataset_root = pathlib.Path('../datasets/')\n",
        "    print('NOT IN COLAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cc2L8BmWiSfc"
      },
      "outputs": [],
      "source": [
        "# Install future annotations for <3.7\n",
        "if sys.version_info < (3,7):\n",
        "    !pip install future-annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GBX0zomgiSfd"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import makassar_ml as ml\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "02ushc2diSfd"
      },
      "outputs": [],
      "source": [
        "class BeijingPM25LightningDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, \n",
        "        root: str, \n",
        "        feature_cols: list[int], \n",
        "        target_cols: list[int], \n",
        "        history: int, \n",
        "        horizon: int, \n",
        "        split: float,\n",
        "        batch_size: int,\n",
        "        ):\n",
        "        self.root = root\n",
        "        self.feature_cols = feature_cols\n",
        "        self.target_cols = target_cols\n",
        "        self.history = history\n",
        "        self.horizon = horizon\n",
        "        self.split = split\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Download the dataset.\n",
        "        ml.datasets.BeijingPM25Dataset(\n",
        "            root=self.root,\n",
        "            download=True,\n",
        "            )\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "\n",
        "        # Create train/val datasets for dataloaders.\n",
        "        if stage == 'fit' or stage is None:\n",
        "            dataset_train_full = ml.datasets.BeijingPM25Dataset(\n",
        "                root=self.root,\n",
        "                download=False,\n",
        "                train=True,\n",
        "                split=self.split,\n",
        "                )\n",
        "            train_n = len(dataset_train_full)\n",
        "            train_val_cutoff = train_n - round(train_n*.25) # 75% train, 25% val\n",
        "\n",
        "            self.dataset_train = torch.utils.data.Subset(dataset_train_full, list(range(0, train_val_cutoff)))\n",
        "            self.dataset_val = torch.utils.data.Subset(dataset_train_full, list(range(train_val_cutoff, train_n)))\n",
        "\n",
        "            self.dataset_train_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
        "                dataset=self.dataset_train,\n",
        "                feature_cols=self.feature_cols,\n",
        "                target_cols=self.target_cols,\n",
        "                history=self.history,\n",
        "                horizon=self.horizon,\n",
        "                )\n",
        "            self.dataset_val_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
        "                dataset=self.dataset_val,\n",
        "                feature_cols=self.feature_cols,\n",
        "                target_cols=self.target_cols,\n",
        "                history=self.history,\n",
        "                horizon=self.horizon,\n",
        "                )\n",
        "\n",
        "        # Create test dataset for dataloaders.\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.dataset_test = ml.datasets.BeijingPM25Dataset(\n",
        "                root=self.root,\n",
        "                download=False,\n",
        "                train=False,\n",
        "                split=self.split,\n",
        "                )\n",
        "            self.dataset_test_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
        "                dataset=self.dataset_test,\n",
        "                feature_cols=self.feature_cols,\n",
        "                target_cols=self.target_cols,\n",
        "                history=self.history,\n",
        "                horizon=self.horizon,\n",
        "                )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self.dataset_train_wrap,\n",
        "            batch_size=self.batch_size,\n",
        "            )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self.dataset_val_wrap,\n",
        "            batch_size=self.batch_size,\n",
        "            )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self.dataset_test_wrap,\n",
        "            batch_size=self.batch_size,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mwwvwFLxiSfe"
      },
      "outputs": [],
      "source": [
        "# Note that for Transformer decoder target mask,\n",
        "# the `length` parameter is the desired length of\n",
        "# the target sequence.\n",
        "# See docs: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward\n",
        "def create_attn_mask(length: int):\n",
        "    \"\"\"Generate mask used for attention mechanisms.\n",
        "\n",
        "    Masks are a lower-triangular matrix of zeros\n",
        "    with the other entries taking value \"-inf\".\n",
        "\n",
        "    Args:\n",
        "        length (int): Length of square-matrix dimension.\n",
        "\n",
        "    Examples:\n",
        "        >>> create_attn_mask(3)\n",
        "        tensor([[0., -inf, -inf],\n",
        "                [0., 0., -inf],\n",
        "                [0., 0., 0.]])\n",
        "    \"\"\"\n",
        "    # Get lower-triangular matrix of ones.\n",
        "    mask = torch.tril(torch.ones(length, length))\n",
        "\n",
        "    # Replace 0 -> \"-inf\" and 1 -> 0.0\n",
        "    mask = (\n",
        "        mask\n",
        "        .masked_fill(mask == 0, float(\"-inf\"))\n",
        "        .masked_fill(mask == 1, float(0.0))\n",
        "    )\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kTNd39bmiSfe"
      },
      "outputs": [],
      "source": [
        "class TimeseriesTransformer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "        n_input_features: int,\n",
        "        n_output_features: int,\n",
        "        d_model: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        batch_first: bool = False,\n",
        "        n_encoder_layers: int = 4,\n",
        "        n_decoder_layers: int = 4,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        # Linear transformation from input-feature space into arbitrary n-dimension space.\n",
        "        # This is similar to a word embedding used in NLP tasks.\n",
        "        self.encoder_projection = torch.nn.Linear(in_features=n_input_features, out_features=d_model)\n",
        "        self.decoder_projection = torch.nn.Linear(in_features=n_output_features, out_features=d_model)\n",
        "\n",
        "        # Transformer encoder/decoder layers.\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=8, # Number of multihead-attention models.\n",
        "            dropout=dropout,\n",
        "            dim_feedforward=4*d_model,\n",
        "            batch_first=batch_first,\n",
        "        )\n",
        "        decoder_layer = torch.nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=8, # Number of multihead-attention models.\n",
        "            dropout=dropout,\n",
        "            dim_feedforward=4*d_model,\n",
        "            batch_first=batch_first,\n",
        "        )\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=n_encoder_layers)\n",
        "        self.decoder = torch.nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers)\n",
        "\n",
        "        # Linear output layer.\n",
        "        # We typically only predict a single data point at a time, so output features is typically 1.\n",
        "        self.linear = torch.nn.Linear(in_features=d_model, out_features=n_output_features)\n",
        "\n",
        "    def encode(self, src: torch.Tensor) -> torch.Tensor:\n",
        "        # Transform source into arbitrary feature space.\n",
        "        x = self.encoder_projection(src)\n",
        "\n",
        "        # # Create source mask.\n",
        "        # if self.batch_first:\n",
        "        #     src_length = src.size(1)\n",
        "        # else:\n",
        "        #     src_length = src.size(0)\n",
        "        # src_mask = create_attn_mask(length=src_length).to(device=self.device)\n",
        "\n",
        "        # Pass the linear transformation through the encoder layers.\n",
        "        # x = self.encoder(x, mask=src_mask)\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def decode(self,\n",
        "        tgt: torch.Tensor,\n",
        "        memory: torch.Tensor,\n",
        "        tgt_mask: torch.Tensor = None,\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"Decode function.\n",
        "\n",
        "        Args:\n",
        "            tgt (torch.Tensor): The sequence to the decoder\n",
        "            memory (torch.Tensor): The sequence from the last layer of the encoder\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Decoded sequence.\n",
        "        \"\"\"\n",
        "        # Transform target into arbitrary feature space.\n",
        "        x = self.decoder_projection(tgt)\n",
        "\n",
        "        # Pass the linear transformation through the decoder layers.\n",
        "        x = self.decoder(tgt=x, memory=memory, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Pass the output of the decoder through the linear prediction layer.\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, \n",
        "        src: torch.Tensor,\n",
        "        tgt: torch.Tensor,\n",
        "        tgt_mask: torch.Tensor = None,\n",
        "        ) -> torch.Tensor:\n",
        "        x = self.encode(src)\n",
        "        y = self.decode(tgt=tgt, memory=x, tgt_mask=tgt_mask)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DY6IgfFtiSff"
      },
      "outputs": [],
      "source": [
        "class BeijingPM25ForecastTransformer(pl.LightningModule):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Create the transformer model.\n",
        "        self.model = TimeseriesTransformer(*args, **kwargs)\n",
        "        \n",
        "        # Save parameters for checkpoint.\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, *args, **kwargs) -> torch.Tensor:\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(\n",
        "            self.model.parameters(), \n",
        "            lr=1e-3, \n",
        "            betas=[0.9, 0.98], \n",
        "            eps=1e-9,\n",
        "            )\n",
        "\n",
        "    def compute_loss(self, y_hat, y):\n",
        "        return self.criterion(y_hat, y)\n",
        "\n",
        "    # Legacy method for reference.\n",
        "    def training_step_old(self, batch, batch_idx):\n",
        "        history_x, history_y, horizon_x, horizon_y = batch\n",
        "        # y_hat = self((history_x, history_y,))\n",
        "        y_hat = self((history_x, horizon_y,))\n",
        "        loss = self.compute_loss(y_hat, horizon_y)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def step(self, batch: torch.Tensor, batch_idx: int, key: str) -> float:\n",
        "        \"\"\"Generic step function used for train/validation/test loops.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.Tensor): Tensor of batched records.\n",
        "            batch_idx (int): Index of batch relative to entire dataset.\n",
        "            key (str): Step key for logging purposes (one of ['train','val','test']).\n",
        "\n",
        "        Returns:\n",
        "            float: Prediction loss.\n",
        "        \"\"\"\n",
        "        history_x, history_y, horizon_x, horizon_y = batch\n",
        "\n",
        "        # Create decoder input sequence.\n",
        "        # This should start with the last element of the encoder sequence\n",
        "        # and end with the second-to-last element of the target sequence.\n",
        "        tgt = torch.cat(\n",
        "            (history_x[:,[-1],:], horizon_x[:,:-1,:]),\n",
        "            dim=1\n",
        "            )\n",
        "        \n",
        "        # Create target attention mask to prevent lookahead cheating.\n",
        "        if self.model.batch_first:\n",
        "            tgt_length = tgt.size(1)\n",
        "        else:\n",
        "            tgt_length = tgt.size(0)\n",
        "        tgt_mask = create_attn_mask(length=tgt_length).to(device=self.device)\n",
        "\n",
        "        # Pass source and target sequences into the transformer.\n",
        "        y_hat = self(history_x, tgt, tgt_mask)\n",
        "\n",
        "        # Compute loss.\n",
        "        # loss = self.compute_loss(y_hat, horizon_y)\n",
        "        loss = self.compute_loss(y_hat, horizon_x)\n",
        "        self.log(f'{key}_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch: torch.Tensor, batch_idx: int) -> float:\n",
        "        return self.step(batch, batch_idx, key='train')\n",
        "\n",
        "    def test_step(self, batch: torch.Tensor, batch_idx: int):\n",
        "        return self.step(batch, batch_idx, key='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qmJYNNj7sN4j"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir ./lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "68b6c76906cd4a129910a6eb69c30036",
            "70f3b0c0c868450caf6246589f17e11f",
            "f551f55819364603a899a92b7dc06fd4",
            "7d6bb509234149c58fde48f03e438025",
            "fedbf5efec724e56aa5c25589b593d06",
            "6098ce04561a4e8488d5a10840e6b665",
            "c5e9e35562504a52941d2fa76aded5f9",
            "0a74d29839de4ba4be8ac9f15b7b033a",
            "c0e9d61bf57a4f02b36e84859b484490",
            "b03afc53a4f24072a99cca671518b395",
            "ed7049fdcd7241ad85f4ffd74cd2a1bb"
          ]
        },
        "id": "2y1QHCUuiSfg",
        "outputId": "8f6374ff-29fa-4026-bfad-1e032f3d3c86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\n",
            "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/configuration_validator.py:120: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
            "\n",
            "  | Name      | Type                  | Params\n",
            "----------------------------------------------------\n",
            "0 | criterion | MSELoss               | 0     \n",
            "1 | model     | TimeseriesTransformer | 29.4 M\n",
            "----------------------------------------------------\n",
            "29.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "29.4 M    Total params\n",
            "117.731   Total estimated model params size (MB)\n",
            "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it, loss=1.01e+06, v_num=, train_loss_step=1.01e+6, train_loss_epoch=1.01e+6]\n"
          ]
        }
      ],
      "source": [
        "# Define parameters for the dataset.\n",
        "feature_cols = [0,1,2,3]\n",
        "target_cols = [-3]\n",
        "history = 5\n",
        "horizon = 3\n",
        "split = 0.15\n",
        "batch_size = 64\n",
        "\n",
        "# Create the dataset.\n",
        "dm = BeijingPM25LightningDataModule(\n",
        "    root=dataset_root,\n",
        "    feature_cols=feature_cols,\n",
        "    target_cols=target_cols,\n",
        "    history=history,\n",
        "    horizon=horizon,\n",
        "    split=split,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "n_input_features: int = len(feature_cols)\n",
        "# n_output_features: int = len(target_cols)\n",
        "n_output_features: int = len(feature_cols)\n",
        "d_model: int = 512\n",
        "dropout: float = 0.2\n",
        "model = BeijingPM25ForecastTransformer(\n",
        "    n_input_features=n_input_features,\n",
        "    n_output_features=n_output_features,\n",
        "    d_model=d_model,\n",
        "    dropout=dropout,\n",
        "    batch_first=True,\n",
        "    n_encoder_layers=4,\n",
        "    n_decoder_layers=4,\n",
        "    )\n",
        "\n",
        "kwargs = {}\n",
        "if torch.cuda.is_available():\n",
        "    kwargs['gpus'] = 1\n",
        "\n",
        "# Devlopment mode.\n",
        "kwargs['fast_dev_run'] = True\n",
        "\n",
        "trainer = pl.Trainer(**kwargs)\n",
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing\n",
        "\n",
        "Here we try to forecast some features and plot them to see how well model does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_loss': 1003820.1875, 'test_loss_epoch': 1003820.1875}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 1003820.1875, 'test_loss_epoch': 1003820.1875}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(model, dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z1PuQBmsiSfh"
      },
      "outputs": [],
      "source": [
        "# dm.prepare_data()\n",
        "# dm.setup()\n",
        "# train = dm.train_dataloader()\n",
        "# val = dm.val_dataloader()\n",
        "# test = dm.test_dataloader()\n",
        "\n",
        "# # Print counts for each split.\n",
        "# print('train',len(train))\n",
        "# print('val',len(val))\n",
        "# print('test',len(test))\n",
        "# print('total',len(train)+len(val)+len(test))\n",
        "\n",
        "# # Visually inspect the split boundaries to ensure that no values are missing.\n",
        "# print('train[0]:',dm.dataset_train[0][0:4])\n",
        "# print('train[-1]:',dm.dataset_train[-1][0:4])\n",
        "# print('val[0]:',dm.dataset_val[0][0:4])\n",
        "# print('val[-1]:',dm.dataset_val[-1][0:4])\n",
        "# print('test[0]:',dm.dataset_test[0][0:4])\n",
        "# print('test[-1]:',dm.dataset_test[-1][0:4])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer_forecast_beijing_pm25.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "73cda5dbe0541404945d78922515ddeb5c5d95f647b0e783aaf5c1a9a2d741fd"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('3.9.2': pyenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a74d29839de4ba4be8ac9f15b7b033a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6098ce04561a4e8488d5a10840e6b665": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b6c76906cd4a129910a6eb69c30036": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f551f55819364603a899a92b7dc06fd4",
              "IPY_MODEL_7d6bb509234149c58fde48f03e438025",
              "IPY_MODEL_fedbf5efec724e56aa5c25589b593d06"
            ],
            "layout": "IPY_MODEL_70f3b0c0c868450caf6246589f17e11f"
          }
        },
        "70f3b0c0c868450caf6246589f17e11f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7d6bb509234149c58fde48f03e438025": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e9d61bf57a4f02b36e84859b484490",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a74d29839de4ba4be8ac9f15b7b033a",
            "value": 1
          }
        },
        "b03afc53a4f24072a99cca671518b395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0e9d61bf57a4f02b36e84859b484490": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e9e35562504a52941d2fa76aded5f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7049fdcd7241ad85f4ffd74cd2a1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f551f55819364603a899a92b7dc06fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5e9e35562504a52941d2fa76aded5f9",
            "placeholder": "​",
            "style": "IPY_MODEL_6098ce04561a4e8488d5a10840e6b665",
            "value": "Epoch 0: 100%"
          }
        },
        "fedbf5efec724e56aa5c25589b593d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed7049fdcd7241ad85f4ffd74cd2a1bb",
            "placeholder": "​",
            "style": "IPY_MODEL_b03afc53a4f24072a99cca671518b395",
            "value": " 1/1 [00:01&lt;00:00,  1.28s/it, loss=1.01e+06, v_num=, train_loss_step=1.01e+6, train_loss_epoch=1.01e+6]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
