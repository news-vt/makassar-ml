{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import makassar_ml as ml\n",
    "import pathlib\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeijingPM25LightningDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "        root: str, \n",
    "        feature_cols: list[int], \n",
    "        target_cols: list[int], \n",
    "        history: int, \n",
    "        horizon: int, \n",
    "        split: float,\n",
    "        batch_size: int,\n",
    "        ):\n",
    "        self.root = root\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.history = history\n",
    "        self.horizon = horizon\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the dataset.\n",
    "        ml.datasets.BeijingPM25Dataset(\n",
    "            root=self.root,\n",
    "            download=True,\n",
    "            )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "\n",
    "        # Create train/val datasets for dataloaders.\n",
    "        if stage == 'fit' or stage is None:\n",
    "            dataset_train_full = ml.datasets.BeijingPM25Dataset(\n",
    "                root=self.root,\n",
    "                download=False,\n",
    "                train=True,\n",
    "                split=self.split,\n",
    "                )\n",
    "            train_n = len(dataset_train_full)\n",
    "            train_val_cutoff = train_n - round(train_n*.25) # 75% train, 25% val\n",
    "\n",
    "            self.dataset_train = torch.utils.data.Subset(dataset_train_full, list(range(0, train_val_cutoff)))\n",
    "            self.dataset_val = torch.utils.data.Subset(dataset_train_full, list(range(train_val_cutoff, train_n)))\n",
    "\n",
    "            self.dataset_train_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
    "                dataset=self.dataset_train,\n",
    "                feature_cols=self.feature_cols,\n",
    "                target_cols=self.target_cols,\n",
    "                history=self.history,\n",
    "                horizon=self.horizon,\n",
    "                )\n",
    "            self.dataset_val_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
    "                dataset=self.dataset_val,\n",
    "                feature_cols=self.feature_cols,\n",
    "                target_cols=self.target_cols,\n",
    "                history=self.history,\n",
    "                horizon=self.horizon,\n",
    "                )\n",
    "\n",
    "        # Create test dataset for dataloaders.\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.dataset_test = ml.datasets.BeijingPM25Dataset(\n",
    "                root=self.root,\n",
    "                download=False,\n",
    "                train=False,\n",
    "                split=self.split,\n",
    "                )\n",
    "            self.dataset_test_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
    "                dataset=self.dataset_test,\n",
    "                feature_cols=self.feature_cols,\n",
    "                target_cols=self.target_cols,\n",
    "                history=self.history,\n",
    "                horizon=self.horizon,\n",
    "                )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.dataset_train_wrap,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.dataset_val_wrap,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.dataset_test_wrap,\n",
    "            batch_size=self.batch_size,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attn_mask(length: int, device: str = None):\n",
    "    \"\"\"Generate mask used for attention mechanisms.\n",
    "\n",
    "    Masks are a lower-triangular matrix of zeros\n",
    "    with the other entries taking value \"-inf\".\n",
    "\n",
    "    Args:\n",
    "        length (int): Length of square-matrix dimension.\n",
    "        device (str, optional): PyTorch device.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> create_mask(3)\n",
    "        tensor([[0., -inf, -inf],\n",
    "                [0., 0., -inf],\n",
    "                [0., 0., 0.]])\n",
    "    \"\"\"\n",
    "    # Get lower-triangular matrix of ones.\n",
    "    mask = torch.tril(torch.ones(length, length, device=device))\n",
    "\n",
    "    # Replace 0 -> \"-inf\" and 1 -> 0.0\n",
    "    mask = (\n",
    "        mask\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesTransformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        n_input_features: int,\n",
    "        n_output_features: int,\n",
    "        d_model: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        batch_first: bool = False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Linear transformation from input-feature space into arbitrary n-dimension space.\n",
    "        # This is similar to a word embedding used in NLP tasks.\n",
    "        self.encoder_projection = torch.nn.Linear(in_features=n_input_features, out_features=d_model)\n",
    "        self.decoder_projection = torch.nn.Linear(in_features=n_output_features, out_features=d_model)\n",
    "\n",
    "        # Transformer encoder/decoder layers.\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=8, # Number of multihead-attention models.\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=4*d_model,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=8, # Number of multihead-attention models.\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=4*d_model,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=8)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=8)\n",
    "\n",
    "        # Linear output layer.\n",
    "        # We typically only predict a single data point at a time, so output features is typically 1.\n",
    "        self.linear = torch.nn.Linear(in_features=d_model, out_features=n_output_features)\n",
    "\n",
    "    def encode(self, src):\n",
    "        # Transform source into arbitrary feature space.\n",
    "        x = self.encoder_projection(src)\n",
    "\n",
    "        # Create source mask.\n",
    "        if self.batch_first:\n",
    "            src_length, batch_size = src.size(1), src.size(0)\n",
    "        else:\n",
    "            src_length, batch_size = src.size(0), src.size(1)\n",
    "        src_mask = create_attn_mask(length=src_length, device=src.device)\n",
    "\n",
    "        # Pass the linear transformation through the encoder layers.\n",
    "        x = self.encoder(x, mask=src_mask)\n",
    "        # x = self.encoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, tgt, memory):\n",
    "        # Transform target into arbitrary feature space.\n",
    "        x = self.decoder_projection(tgt)\n",
    "\n",
    "        # Create target attention mask.\n",
    "        if self.batch_first:\n",
    "            tgt_length, batch_size = tgt.size(1), tgt.size(0)\n",
    "        else:\n",
    "            tgt_length, batch_size = tgt.size(0), tgt.size(1)\n",
    "\n",
    "        tgt_mask = create_attn_mask(length=tgt_length, device=tgt.device)\n",
    "\n",
    "        # Pass the linear transformation through the decoder layers.\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=tgt_mask)\n",
    "\n",
    "        # Pass the output of the decoder through the linear prediction layer.\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        src, tgt = x\n",
    "        y = self.encode(src)\n",
    "        y = self.decode(tgt=tgt, memory=y)\n",
    "        return y\n",
    "\n",
    "    def step(self, batch):\n",
    "        src, tgt_int, tgt_out = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeijingPM25ForecastTransformer(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "        n_input_features: int,\n",
    "        n_output_features: int,\n",
    "        d_model: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "        # Create the transformer model.\n",
    "        self.model = TimeseriesTransformer(\n",
    "            n_input_features=n_input_features,\n",
    "            n_output_features=n_output_features,\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        return self.criterion(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        history_x, history_y, horizon_x, horizon_y = batch\n",
    "        # y_hat = self((history_x, history_y,))\n",
    "        y_hat = self((history_x, horizon_y,))\n",
    "        loss = self.compute_loss(y_hat, horizon_y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/configuration_validator.py:120: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name      | Type                  | Params\n",
      "----------------------------------------------------\n",
      "0 | criterion | MSELoss               | 0     \n",
      "1 | model     | TimeseriesTransformer | 58.9 M\n",
      "----------------------------------------------------\n",
      "58.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.9 M    Total params\n",
      "235.422   Total estimated model params size (MB)\n",
      "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|‚ñç         | 34/873 [00:36<15:07,  1.08s/it, loss=2.28e+03, v_num=31, train_loss_step=141.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derieux/My Drive/Virginia Tech/graduate/research/makassar/repos/makassar-ml/__pypackages__/3.9/lib/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the dataset.\n",
    "root = pathlib.Path('../datasets/')\n",
    "feature_cols = [0,1,2,3]\n",
    "target_cols = [-3]\n",
    "history = 5\n",
    "horizon = 3\n",
    "split = 0.15\n",
    "batch_size = 32\n",
    "\n",
    "# Create the dataset.\n",
    "dm = BeijingPM25LightningDataModule(\n",
    "    root=root,\n",
    "    feature_cols=feature_cols,\n",
    "    target_cols=target_cols,\n",
    "    history=history,\n",
    "    horizon=horizon,\n",
    "    split=split,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "n_input_features: int = len(feature_cols)\n",
    "n_output_features: int = len(target_cols)\n",
    "d_model: int = 512\n",
    "dropout: float = 0.1\n",
    "model = BeijingPM25ForecastTransformer(\n",
    "    n_input_features=n_input_features,\n",
    "    n_output_features=n_output_features,\n",
    "    d_model=d_model,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 27931\n",
      "val 9305\n",
      "test 6567\n",
      "total 43803\n",
      "train[0]: tensor([2.0100e+03, 1.0000e+00, 1.0000e+00, 0.0000e+00], dtype=torch.float64)\n",
      "train[-1]: tensor([2.0130e+03, 3.0000e+00, 1.0000e+01, 1.0000e+00], dtype=torch.float64)\n",
      "val[0]: tensor([2.0130e+03, 3.0000e+00, 1.0000e+01, 2.0000e+00], dtype=torch.float64)\n",
      "val[-1]: tensor([2.0140e+03, 4.0000e+00, 2.0000e+00, 1.0000e+00], dtype=torch.float64)\n",
      "test[0]: tensor([2.0140e+03, 4.0000e+00, 2.0000e+00, 2.0000e+00], dtype=torch.float64)\n",
      "test[-1]: tensor([2014.,   12.,   31.,   23.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# dm.prepare_data()\n",
    "# dm.setup()\n",
    "# train = dm.train_dataloader()\n",
    "# val = dm.val_dataloader()\n",
    "# test = dm.test_dataloader()\n",
    "\n",
    "# # Print counts for each split.\n",
    "# print('train',len(train))\n",
    "# print('val',len(val))\n",
    "# print('test',len(test))\n",
    "# print('total',len(train)+len(val)+len(test))\n",
    "\n",
    "# # Visually inspect the split boundaries to ensure that no values are missing.\n",
    "# print('train[0]:',dm.dataset_train[0][0:4])\n",
    "# print('train[-1]:',dm.dataset_train[-1][0:4])\n",
    "# print('val[0]:',dm.dataset_val[0][0:4])\n",
    "# print('val[-1]:',dm.dataset_val[-1][0:4])\n",
    "# print('test[0]:',dm.dataset_test[0][0:4])\n",
    "# print('test[-1]:',dm.dataset_test[-1][0:4])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73cda5dbe0541404945d78922515ddeb5c5d95f647b0e783aaf5c1a9a2d741fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
