{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import makassar_ml as ml\n",
    "import pathlib\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeijingPM25LightningModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "        root: str, \n",
    "        feature_cols: list[int], \n",
    "        target_cols: list[int], \n",
    "        history: int, \n",
    "        horizon: int, \n",
    "        split: float,\n",
    "        batch_size: int,\n",
    "        ):\n",
    "        self.root = root\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.history = history\n",
    "        self.horizon = horizon\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the dataset.\n",
    "        ml.datasets.BeijingPM25Dataset(\n",
    "            root=self.root,\n",
    "            download=True,\n",
    "            )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "\n",
    "        # Create train/val datasets for dataloaders.\n",
    "        if stage == 'fit' or stage is None:\n",
    "            dataset_train_full = ml.datasets.BeijingPM25Dataset(\n",
    "                root=self.root,\n",
    "                download=False,\n",
    "                train=True,\n",
    "                split=self.split,\n",
    "                )\n",
    "            train_n = len(dataset_train_full)\n",
    "            train_val_cutoff = train_n - round(train_n*.25) # 75% train, 25% val\n",
    "\n",
    "            self.dataset_train = torch.utils.data.Subset(dataset_train_full, list(range(0, train_val_cutoff)))\n",
    "            self.dataset_val = torch.utils.data.Subset(dataset_train_full, list(range(train_val_cutoff, train_n)))\n",
    "\n",
    "            self.dataset_train_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
    "                dataset=self.dataset_train,\n",
    "                feature_cols=self.feature_cols,\n",
    "                target_cols=self.target_cols,\n",
    "                history=self.history,\n",
    "                horizon=self.horizon,\n",
    "                )\n",
    "            self.dataset_val_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
    "                dataset=self.dataset_val,\n",
    "                feature_cols=self.feature_cols,\n",
    "                target_cols=self.target_cols,\n",
    "                history=self.history,\n",
    "                horizon=self.horizon,\n",
    "                )\n",
    "\n",
    "        # Create test dataset for dataloaders.\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.dataset_test = ml.datasets.BeijingPM25Dataset(\n",
    "                root=self.root,\n",
    "                download=False,\n",
    "                train=False,\n",
    "                split=self.split,\n",
    "                )\n",
    "            self.dataset_test_wrap = ml.datasets.TimeseriesForecastDatasetWrapper(\n",
    "                dataset=self.dataset_test,\n",
    "                feature_cols=self.feature_cols,\n",
    "                target_cols=self.target_cols,\n",
    "                history=self.history,\n",
    "                horizon=self.horizon,\n",
    "                )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.dataset_train_wrap,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.dataset_val_wrap,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self.dataset_test_wrap,\n",
    "            batch_size=self.batch_size,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the dataset.\n",
    "root = pathlib.Path('../datasets/')\n",
    "feature_cols = [0,1,2,3]\n",
    "target_cols = [-3]\n",
    "history = 5\n",
    "horizon = 3\n",
    "split = 0.15\n",
    "batch_size = 1\n",
    "\n",
    "# Create the dataset.\n",
    "dm = BeijingPM25LightningModule(\n",
    "    root=root,\n",
    "    feature_cols=feature_cols,\n",
    "    target_cols=target_cols,\n",
    "    history=history,\n",
    "    horizon=horizon,\n",
    "    split=split,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 27931\n",
      "val 9305\n",
      "test 6567\n",
      "total 43803\n",
      "train[0]: tensor([2.0100e+03, 1.0000e+00, 1.0000e+00, 0.0000e+00], dtype=torch.float64)\n",
      "train[-1]: tensor([2.0130e+03, 3.0000e+00, 1.0000e+01, 1.0000e+00], dtype=torch.float64)\n",
      "val[0]: tensor([2.0130e+03, 3.0000e+00, 1.0000e+01, 2.0000e+00], dtype=torch.float64)\n",
      "val[-1]: tensor([2.0140e+03, 4.0000e+00, 2.0000e+00, 1.0000e+00], dtype=torch.float64)\n",
      "test[0]: tensor([2.0140e+03, 4.0000e+00, 2.0000e+00, 2.0000e+00], dtype=torch.float64)\n",
      "test[-1]: tensor([2014.,   12.,   31.,   23.], dtype=torch.float64)\n",
      "test[-1]: tensor([2014.0000,   12.0000,   31.0000,   23.0000,   12.0000,  -21.0000,\n",
      "          -3.0000, 1034.0000,  249.8500,    0.0000,    0.0000],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "train = dm.train_dataloader()\n",
    "val = dm.val_dataloader()\n",
    "test = dm.test_dataloader()\n",
    "\n",
    "# Print counts for each split.\n",
    "print('train',len(train))\n",
    "print('val',len(val))\n",
    "print('test',len(test))\n",
    "print('total',len(train)+len(val)+len(test))\n",
    "\n",
    "# Visually inspect the split boundaries to ensure that no values are missing.\n",
    "print('train[0]:',dm.dataset_train[0][0:4])\n",
    "print('train[-1]:',dm.dataset_train[-1][0:4])\n",
    "print('val[0]:',dm.dataset_val[0][0:4])\n",
    "print('val[-1]:',dm.dataset_val[-1][0:4])\n",
    "print('test[0]:',dm.dataset_test[0][0:4])\n",
    "print('test[-1]:',dm.dataset_test[-1][0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BeijingPM25ForecastTransformer(pl.LightningModule):\n",
    "#     def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "#         super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73cda5dbe0541404945d78922515ddeb5c5d95f647b0e783aaf5c1a9a2d741fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
