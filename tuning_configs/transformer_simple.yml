# Dataset.
dataset:
  name: beijingpm25
  parameters:
    path: ${HOME}/research/makassar/datasets/beijing_pm25
    in_feat: &ID_in_feat ['day_of_year','TEMP','PRES','Iws','Is','Ir']
    out_feat: &ID_out_feat ['DEWP']
    in_seq_len: &ID_in_seq_len 24 # Alias for future use.
    out_seq_len: &ID_out_seq_len 1 # Alias for future use.
    shift: 1
    split: [0.7,0.2,0.1]
    shuffle: False

# Model definition.
model:
  name: transformer_simple
  parameters:
    in_seq_len: *ID_in_seq_len
    in_feat: !len [*ID_in_feat]
    out_feat: !len [*ID_out_feat]
    embed_dim: 
      values:
        - 32
    n_heads: 
      values:
        - 8
    ff_dim:
      values:
        - 256
    dropout: 
      values:
        - 0.1
    n_encoders:
      values:
        - 3
    embed_type:
      values:
        - linear
        - t2v
    norm_type:
      values:
        - batch
        - layer

# Training configuration.
train:
  batch_size: 128
  epochs: 10

  # Optimizer.
  optimizer:
    name: adam
    parameters:
      lr: 0.0001

  # Compile parameters.
  compile:
    loss: mse
    metrics: ['mae','mape']

  # Training callbacks.
  callbacks:
    EarlyStopping:
      monitor: val_loss
      mode: auto
      patience: 3
      restore_best_weights: True
    # LearningRateAdjuster: {}
    LearningRateScheduler:
      schedule: lr_scheduler_linear_warmup_linear_decay
      parameters:
        warmup_epochs: 15
        decay_epochs: 100
        initial_lr: 1.0e-6
        base_lr: 1.0e-4
        min_lr: 5.0e-5

# Root directory paths.
roots:
  project_root: &ID_project_root ${HOME}/research/makassar/
  checkpoint_root: !join [*ID_project_root, checkpoints]
  image_root: !join [*ID_project_root, images]
  table_root: !join [*ID_project_root, tables]
  hp_tuning_root: !join [*ID_project_root, hp_tuning]
  keras_tuner_path: !join [*ID_project_root, keras_tuner]