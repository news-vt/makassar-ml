# Dataset.
dataset:
  name: beijingpm25
  parameters:
    path: ${HOME}/research/makassar/datasets/beijing_pm25
    in_feat: &ID_in_feat ['day_of_year','TEMP','Iws','Is','Ir']
    out_feat: &ID_out_feat ['DEWP','PRES']
    in_seq_len: &ID_in_seq_len 168 # 1 week
    out_seq_len: &ID_out_seq_len 1 # 1 hour
    shift: 1
    split: [0.7,0.2,0.1]
    shuffle: False

# Model definition.
model:
  name: FoT
  parameters:
    in_seq_len: *ID_in_seq_len
    in_feat: !len [*ID_in_feat]
    out_feat: !len [*ID_out_feat]
    embed_dim: 
      values:
        - 16
        - 32
    n_heads: 
      values:
        - 8
    ff_dim:
      values:
        - 256
        - 512
    dropout: 
      values:
        - 0.1
        - 0.3
    n_encoders:
      values:
        - 3
        - 6

# Training configuration.
train:
  batch_size: 128
  epochs: 30

  # Optimizer.
  optimizer:
    name: adam
    parameters:
      # lr: 1.0e-4
      lr:
        # CosineDecay:
        #   initial_learning_rate: 1.0e-3
        #   decay_steps: 500
        #   alpha: 0.0
        CosineDecayRestarts:
          initial_learning_rate: 1.0e-3
          first_decay_steps: 1000
          alpha: 0.0


  # Compile parameters.
  compile:
    loss: mse
    metrics: ['mae']

  # # Training callbacks.
  # callbacks:
  #   # EarlyStopping:
  #   #   monitor: val_loss
  #   #   mode: auto
  #   #   patience: 3
  #   #   restore_best_weights: True
  #   # LearningRateAdjuster: {}
  #   # LearningRateScheduler:
  #   #   schedule: CosineDecay
  #   #   parameters:
  #   #     initial_learning_rate: 1.0e-5
  #   #     decay_epochs: 20
  #   #     alpha: 0.0

# Root directory paths.
roots:
  project_root: &ID_project_root ${HOME}/research/makassar/
  checkpoint_root: !join [*ID_project_root, checkpoints]
  image_root: !join [*ID_project_root, images]
  table_root: !join [*ID_project_root, tables]
  hp_tuning_root: !join [*ID_project_root, hp_tuning]
  keras_tuner_path: !join [*ID_project_root, keras_tuner]